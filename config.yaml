# data
train_file: './ft_local/data-with-body/data.train.json'
valid_file: './ft_local/data-with-body/data.dev.json'
test_file: './ft_local/data-with-body/data.test.json'
vocab_file: './ft_local/data-with-body/vocab.txt'
vocab_size: 30000

# model
#type_num: 4
#head_num: 4
emb_size: 256
encoder_hidden_size: 128
decoder_hidden_size: 256
num_layers: 2
bidirec: True
shared_vocab: True
dropout: 0.1
max_tgt_len: 50
#max_sentence_len: 100

# specific
tau: 0.5
gama1: 0.0
gama2: 0.0

# optimize
epoch: 30
batch_size: 64
param_init: 0.01
optim: 'adam'
learning_rate: 0.0005
max_grad_norm: 8
window_size: [1,2,3]
filter_size: 128
learning_rate_decay: 0.5
schedule: True
start_decay_at: 2

# log
log: './log/'
eval_interval: 10000
save_interval: 10000
metric: ['bleu', 'xent']

# generate
max_generator_batches: 32
beam_size: 5
